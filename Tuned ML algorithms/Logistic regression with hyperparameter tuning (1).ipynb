{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d545a2",
   "metadata": {},
   "source": [
    "# Applying hyperparameter tuning to already created logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67cb35d",
   "metadata": {},
   "source": [
    "### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6bba75",
   "metadata": {},
   "source": [
    "In the logistic regression from scratch file, I had implemented a model which gave a linear decision boundary and after that I had also implemented the model that gave a quadratic boundary which helped increase accuracy further\n",
    "\n",
    "In this file we will try to improve upon the linear logistic regression model by implementing it again but this time with hyperparameter tuning using random search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2937936",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d4a470",
   "metadata": {},
   "source": [
    "Lets start\n",
    "\n",
    "First import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26eb133a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6bf98",
   "metadata": {},
   "source": [
    "defining the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6946cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression():\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, iter_num=1000, reg_strength=0, early_stopping=False, early_stopping_tol=1e-4, early_stopping_patience=5, weight_init='zeros'):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iter_num = iter_num\n",
    "        self.reg_strength = reg_strength\n",
    "        self.early_stopping = early_stopping\n",
    "        self.early_stopping_tol = early_stopping_tol\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.weight_init = weight_init\n",
    "        self.losses = []\n",
    "        self.best_weights = None\n",
    "\n",
    "    def fit(self, X, Y, X_val=None, Y_val=None):\n",
    "        self.m, self.n = X.shape\n",
    "        self.w, self.b = self.initialize_weights()\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "        if self.early_stopping:\n",
    "            best_loss = np.inf\n",
    "            patience_count = 0\n",
    "\n",
    "        for i in range(self.iter_num):\n",
    "            self.update_weights()\n",
    "            loss = self.compute_loss()\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            if self.early_stopping and X_val is not None and Y_val is not None:\n",
    "                val_loss = self.compute_loss(X_val, Y_val)\n",
    "                if val_loss < best_loss - self.early_stopping_tol:\n",
    "                    best_loss = val_loss\n",
    "                    patience_count = 0\n",
    "                    self.best_weights = self.w, self.b\n",
    "                else:\n",
    "                    patience_count += 1\n",
    "                    if patience_count >= self.early_stopping_patience:\n",
    "                        break\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        if self.weight_init == 'zeros':\n",
    "            return np.zeros(self.n), 0\n",
    "        elif self.weight_init == 'random':\n",
    "            return np.random.randn(self.n), 0\n",
    "\n",
    "    def update_weights(self):\n",
    "        Y_hat = self.predict_proba(self.X)\n",
    "        dw = (1 / self.m) * np.dot(self.X.T, (Y_hat - self.Y))\n",
    "        db = (1 / self.m) * np.sum(Y_hat - self.Y)\n",
    "\n",
    "        dw += (self.reg_strength / self.m) * self.w\n",
    "\n",
    "        self.w = self.w - self.learning_rate * dw\n",
    "        self.b = self.b - self.learning_rate * db\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return 1 / (1 + np.exp(-(X.dot(self.w) + self.b)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(self.predict_proba(X) > 0.5, 1, 0)\n",
    "\n",
    "    def compute_loss(self, X=None, Y=None):\n",
    "        if X is None and Y is None:\n",
    "            X, Y = self.X, self.Y\n",
    "\n",
    "        Y_hat = self.predict_proba(X)\n",
    "        loss = (-Y * np.log(Y_hat) - (1 - Y) * np.log(1 - Y_hat)).mean()\n",
    "\n",
    "        loss += 0.5 * (self.reg_strength / self.m) * np.sum(self.w**2)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e70751b",
   "metadata": {},
   "source": [
    "defining the 'man of the match': the random search code snippet is shown as follows\n",
    "\n",
    "Its a pretty simple implementation\n",
    "\n",
    "Detailed comments have been provided in this code cell to make it crystal clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "584d1fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Parameters of the function:\n",
    "        X_train ndarray: Training feature data.\n",
    "        Y_train ndarray: Training label data.\n",
    "        X_val ndarray: Validation/test feature data.\n",
    "        Y_val ndarray: Validation/testing label data.\n",
    "        hyperparameters dict: Dictionary containing hyperparameter names as keys and their possible values as lists.\n",
    "        num_combinations (int, optional): Number of random hyperparameter combinations to try. Default is 10.\n",
    "\n",
    "    Return type of the function:\n",
    "        tuple: A tuple containing the best set of hyperparameters and the corresponding accuracy on the validation set.\n",
    "    \"\"\"\n",
    "\n",
    "def random_search(X_train, Y_train, X_val, Y_val, hyperparameters, num_combinations=10):\n",
    "    best_hyperparams = None\n",
    "    best_accuracy = -np.inf\n",
    "    \n",
    "    # Iterate over num_combinations random hyperparameter combinations\n",
    "    for _ in range(num_combinations):\n",
    "        # Randomly sample hyperparameters from the hyperparameter space\n",
    "        params = {k: np.random.choice(v) for k, v in hyperparameters.items()}\n",
    "        \n",
    "        # Create a logistic regression model with the sampled hyperparameters\n",
    "        model = Logistic_Regression(**params)\n",
    "        \n",
    "        # Train the model on the training dataset\n",
    "        model.fit(X_train, Y_train, X_val, Y_val)\n",
    "        \n",
    "        # Evaluate the model's accuracy on the validation dataset\n",
    "        accuracy = np.mean(model.predict(X_val) == Y_val)\n",
    "        \n",
    "        # Update the best hyperparameters and accuracy if the current model performs better\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_hyperparams = params\n",
    "\n",
    "    return best_hyperparams, best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d7136",
   "metadata": {},
   "source": [
    "This point onwards the code is pretty simple and straightforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53a39ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train dataset\n",
    "train_dataset = pd.read_csv('ds1_train.csv')\n",
    "train_dataset.head()\n",
    "\n",
    "# Extract the feature columns (X) and the output label column (Y)\n",
    "X_train = train_dataset[['x_1', 'x_2']].values\n",
    "Y_train = train_dataset['y'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609d0095",
   "metadata": {},
   "source": [
    "Note: Unlike last time in the scratch file, here i have normalised using standard deviation, for better normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c15b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features using standardization \n",
    "# standardization is normalisation with use of standard deviation and not Xmax and Xmin in the denominator\n",
    "def standardize_features(X):\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    X_std = np.std(X, axis=0)\n",
    "    X_standardized = (X - X_mean) / X_std\n",
    "    return X_standardized\n",
    "\n",
    "X_train = standardize_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02901cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation dataset \n",
    "val_dataset = pd.read_csv('ds1_test.csv')\n",
    "X_val = val_dataset[['x_1', 'x_2']].values\n",
    "Y_val = val_dataset['y'].values\n",
    "\n",
    "# Normalize the features for validation data using the same standardization parameters from training data\n",
    "X_val = standardize_features(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7218a2",
   "metadata": {},
   "source": [
    "The code block below is the main block where all the magic happens\n",
    "\n",
    "Hyperparameter choosable values are set which are then passed to random search to find their possible best combination\n",
    "\n",
    "Then that combination gets fit to the model at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eec3584a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.01, 'iter_num': 50000, 'reg_strength': 1.0, 'early_stopping': True, 'weight_init': 'random'}\n",
      "Best Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters to tune being set to be chosen by the random search model\n",
    "hyperparameters = {\n",
    "    'learning_rate': [0.01, 0.1,1],\n",
    "    'iter_num': [20000, 25000,10000,100000,50000],\n",
    "    'reg_strength': [0, 0.01, 0.1, 1],\n",
    "    'early_stopping': [False,True],\n",
    "    'weight_init': ['zeros', 'random']\n",
    "}\n",
    "\n",
    "# Perform random search to find the best hyperparameters\n",
    "best_hyperparams, best_accuracy = random_search(X_train, Y_train, X_val, Y_val, hyperparameters, num_combinations=20)\n",
    "\n",
    "# we have set a big variety of different hyperparameter values to check for random search\n",
    "# also we have chosen 20 combinations to be random searched through\n",
    "# hence the model takes some time to train\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_hyperparams)\n",
    "print(\"Best Accuracy:\", best_accuracy)\n",
    "\n",
    "# Fit the model with the best hyperparameters on the entire training dataset\n",
    "best_model = Logistic_Regression(**best_hyperparams)\n",
    "best_model.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ecd1d",
   "metadata": {},
   "source": [
    "This is the best output as you can see: \n",
    "\n",
    "\"Best Hyperparameters: {'learning_rate': 0.01, 'iter_num': 50000, 'reg_strength': 1.0, 'early_stopping': True, 'weight_init': 'random'}\n",
    "Best Accuracy: 0.85 \"\n",
    "\n",
    "Hence a 2% increase has occured in the linear decision boundary model after applying random search and tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698d3006",
   "metadata": {},
   "source": [
    "Quite a lot of reasons have caused the increment to be not quite big. Some of them which I suspect are the case here:\n",
    "\n",
    "* the amount of training data is not big enough.\n",
    "* avoiding using feature engineering to increase accuracy as it technically is not a hyperparameter, but rather a technique to enhance the model by adding more levels of complexity in the decision boundary and hence in the overall model. Also utilizing polynomial logistic regression is not so easy as there is a risk of overfitting quite easily. Hence proper preprocessing, and technique usage like regularisation has to be executed in order to ensure that technique's success.\n",
    "* also the model is a simple logistic regression which isnt the best classifier model out there\n",
    "* the accuracy for the simple model was already near its peak value due to me manually setting good values by hit and trial to get best output in the scratch code, and so not much increment could be achieved due to the sheer less room left for improvement\n",
    "* I am not a professional, as I am also currently learning and am fairly new to the topics of ML and DL, so I am not qualified enough or skillful enough to write better code than the one I am currently providing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f5db2e",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "Since the random search uses a different seed each time it's code block is run, there was one instance where the accuracy i achieved was 89% with certain parameter combinations. but i didnt copy paste that output. Hence after realising that everytime randomsearch will give different best outputs, I decided to just copy paste the best output I currently got, as shown in the markdown cells above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43b98a",
   "metadata": {},
   "source": [
    "# Thank you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e8d1cb",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
